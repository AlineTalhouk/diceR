#' External validity indices
#' 
#' \strong{E}xternal \strong{v}alidity indices compare a predicted clustering 
#' result with a reference class or gold standard.
#' 
#' \code{ev_accuracy} computes the classification accuracy for a clustering
#' result
#' 
#' @param pred.lab predicted labels generated by classifier
#' @param ref.lab reference labels for the observations
#' @references MATLAB function \code{valid_CA} and \code{valid_RandIndex} in
#'   package LinkCluE by Simon Garrett
#' @references Strehl A, Ghosh J. Cluster ensembles: a knowledge reuse framework
#'   for combining multiple partitions. J. Mach. Learn. Res. 2002;3:583-617.
#' @return \code{ev_accuracy} returns the classification accuracy
#' @author Johnson Liu, Derek Chiu
#' @name external_validity
#' @export
#' 
#' @examples
#' set.seed(1)
#' E <- matrix(rep(sample(1:4, 1000, replace = TRUE)), nrow = 100, byrow =
#'               FALSE)
#' ev_accuracy(E[, 1], E[, 2])
#' ev_rand(E[, 1], E[, 2])
#' set.seed(4)
#' X <- sample(1:4, 100, replace = TRUE)
#' Y <- sample(1:4, 100, replace = TRUE)
#' ev_nmi(X, Y)
ev_accuracy <- function(pred.lab, ref.lab) {
  assertthat::assert_that(is.vector(pred.lab), is.vector(ref.lab))
  nrow <- length(ref.lab)
  C <- sort(unique(pred.lab))
  k <- length(C)
  clusters <- list(NULL)
  for (i in 2:k) {
    clusters <- c(clusters, NULL)
  }
  ca <- 0
  for (i in 1:k) {
    ind <- which(pred.lab == C[i])
    clusters[[i]] <- matrix(ind, ncol = 1)
    n <- length(ind)
    temp <- NULL
    for (j in 1:n) {
      temp <- append(temp, ref.lab[ind[j]])
    }
    clusters[[i]] <- cbind(clusters[[i]], temp)
    TC <- sort(unique(clusters[[i]][, 2]))
    kTC <- length(TC)
    ind <- NULL
    for (l in 1:kTC) {
      ind <- append(ind, length(which(clusters[[i]][, 2] == TC[l])))
    }
    M <- max(ind)
    I <- which(ind == M)
    ca <- ca + M
  }
  return(ca / nrow)
}

#' @details \code{ev_rand} calculates Rand indices (and their variants) to
#'   compare two partitions
#' @return \code{ev_rand} returns a list with elements
#'   \item{AR}{adjusted Rand index}
#'   \item{RI}{unadjusted Rand index}
#'   \item{MI}{Mirkin's index}
#'   \item{HI}{Hubert's index}
#' @rdname external_validity
#' @export
ev_rand <- function(pred.lab, ref.lab) {
  assertthat::assert_that(is.vector(pred.lab))
  assertthat::assert_that(is.vector(ref.lab))
  C <- table(pred.lab, ref.lab)
  n <- sum(C)
  nis <- sum(rowSums(C) ^ 2)
  njs <- sum(colSums(C) ^ 2)
  t1 <- n * (n - 1) / 2
  t2 <- sum(C ^ 2)
  t3 <- 0.5 * (nis + njs)
  nc <- (n * (n ^ 2 + 1) - (n + 1) * nis - (n + 1) * njs + 2 * (nis * njs) /
           n) / (2 * (n - 1))
  A <- t1 + t2 - t3
  D <- t3 - t2
  if (t1 == nc) {
    AR <- 0
  } else{
    AR <- (A - nc) / (t1 - nc)
  }
  RI <- A / t1
  MI <- D / t1
  HI <- (A - D) / t1
  return(list(
    AR = AR,
    RI = RI,
    MI = MI,
    HI = HI
  ))
}

#' @details \code{ev_nmi} calculates the normalized mutual information
#'   
#' @note \code{ev_nmi} is adapted from \code{\link[infotheo]{mutinformation}}
#' @param method method of computing the entropy. Can be any one of "emp", "mm",
#'   "shrink", or "sg".
#' @rdname external_validity
#' @export
ev_nmi <- function(pred.lab, ref.lab, method = "emp") {
  U <- data.frame(ref.lab, pred.lab)
  Hyx <- infotheo::entropy(U, method)
  Hx <- infotheo::entropy(pred.lab, method)
  Hy <- infotheo::entropy(ref.lab, method)
  I <- ifelse(Hx + Hy - Hyx < 0, 0, Hx + Hy - Hyx)
  NMI <- I / sqrt(Hx * Hy)
  return(NMI)
}

#' Classification Accuracy
#'
#' Given a sorted table, the accuracy is calculated based on proportion
#' correctly classified.
#'
#' The table must have rows and columns corresponding to the same classes. Then
#' the classification accuracy is computed as the sum of the diagonal entries
#' divided by the sum of all entries in the confusion matrix comparing the
#' predicted and reference classes.
#'
#' @param tbl table with predicted and reference classes correctly matched
#' @return classification accuracy, given as a proportion
#' @author Derek Chiu
#' @export
#' @examples
#' set.seed(1)
#' x <- matrix(rbinom(16, 20, 0.4), nrow = 4)
#' accuracy(x)
accuracy <- function(tbl) {
  return(sum(diag(tbl)) / sum(tbl))
}