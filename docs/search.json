[{"path":"https://alinetalhouk.github.io/diceR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2020 Derek Chiu Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Cluster Analysis using `diceR`","text":"Cluster analysis way “slicing dicing” data allow grouping together similar entities separation dissimilar ones. Issues arise due existence diverse number clustering algorithms, different techniques inputs, universally optimal methodology. Thus, framework cluster analysis validation methods needed. approach use cluster ensembles diverse set algorithms. ensures data considered several angles using variety methods. currently implemented 15 clustering algorithms, provide simple framework add additional algorithms (see example(\"consensus_cluster\")). Although results dependent subset algorithms chosen ensemble, intent capture variety clustering outputs select consistent data. can install diceR CRAN : get latest development version GitHub: load excerpt expression data set TCGA 100 high grade serous carcinoma samples measured 50 genes.","code":"install.packages(\"diceR\") # install.packages(\"devtools\") devtools::install_github(\"AlineTalhouk/diceR\") library(diceR) library(dplyr) library(ggplot2) library(pander) data(hgsc) hgsc <- hgsc[1:100, 1:50]"},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"clustering-algorithms","dir":"Articles","previous_headings":"","what":"Clustering Algorithms","title":"Cluster Analysis using `diceR`","text":"list clustering algorithms available use : \"nmf\": Nonnegative Matrix Factorization (using Kullback-Leibler Divergence Euclidean distance) \"hc\": Hierarchical Clustering \"diana\": DIvisive ANAlysis Clustering \"km\": K-Means Clustering \"pam\": Partition Around Medoids \"ap\": Affinity Propagation \"sc\": Spectral Clustering using Radial-Basis kernel function \"gmm\": Gaussian Mixture Model using Bayesian Information Criterion EM algorithm \"block\": Biclustering using latent block model \"som\": Self-Organizing Map (SOM) Hierarchical Clustering \"cmeans\": Fuzzy C-Means Clustering \"hdbscan\": Hierarchical Density-based Spatial Clustering Applications Noise (HDBSCAN) passed character vector algorithms parameter consensus_cluster(). Note list continues evolve implement algorithms diceR.","code":""},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"cluster-models","dir":"Articles","previous_headings":"Clustering Algorithms","what":"Cluster Models","title":"Cluster Analysis using `diceR`","text":"clustering algorithms can categorized different models paradigms based clusters formed. provide brief overview guide initial selection algorithms since single algorithm works every data model.1 Connectivity Models: \"hc\" \"diana\" hierarchical models based connecting objects using particular similarity distance metric. Centroid Models: \"km\" \"pam\" uses initial cluster centers calculated based mean objects, \"pam\" restricts centroid actual object. models good modeling data produces spherical clusters. \"ap\" similar \"pam\" calculates “exemplars” (like centroids) data advantageous initialization k (number clusters) required.2\"cmeans\" similar \"km\" except points can assigned one cluster. Distribution Models: \"gmm\" assumes Gaussian mixture model used EM algorithm clustering.3 gain model complexity traded utility since assumption quite strong. Density Models: \"hdbscan\" runs hierarchical clustering DBSCAN result.4,5 models assume cluster high density, overlapping regions, algorithms may able separate objects well. Subspace Models: \"block\" clusters samples features, useful want associate pairs cluster entities (e.g. functional gene groups associate sample subtypes?). implementation uses latent block model.6 Neural Models: implementation \"som\" reduces data two-dimensional subspace using neural network model, performing hierarchical clustering.7 lot noise phased cluster stability can improved using \"hc\" alone. Spectral Clustering: Similar \"som\", \"sc\" also performs dimensionality reduction clustering, instead uses eigenvectors kernel matrix.8 use radial-basis kernel function package. Non-negative Matrix Factorization: computational complexity \"nmf\" doubled compared algorithms data manipulations need performed transform input data non-negative matrix. Otherwise, NMF natural clustering result can used group together samples /features.9","code":""},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"distance-measures","dir":"Articles","previous_headings":"","what":"Distance Measures","title":"Cluster Analysis using `diceR`","text":"list distance measures available use found stats::dist spearman distance: \"euclidean\": see stats::dist \"maximum\": see stats::dist \"manhattan\": see stats::dist \"canberra\": see stats::dist \"binary\": see stats::dist \"minkowski\": see stats::dist \"spearman\": based bioDist::spearman.dist passed character vector distance parameter consensus_cluster(). \"hc\", \"diana\", \"pam\" algorithms use distance parameter.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"internal","dir":"Articles","previous_headings":"Validity Indices","what":"Internal","title":"Cluster Analysis using `diceR`","text":"list internal validity indices used evaluating clusters : Calinski-Harabasz, Dunn, PBM, Tau, Gamma, C-index, Davies-Bouldin, McClain-Rao, SD, Ray-Turi, G-plus, Rousseeuw’s Silhouette, S-Dbw clusterCri R package Dunn index Connectivity measure clValid R package Compactness measure LinkCluE MATLAB package","code":""},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"external","dir":"Articles","previous_headings":"Validity Indices","what":"External","title":"Cluster Analysis using `diceR`","text":"list external validity indices used evaluating clusters : Hubert, Jaccard, McNemar, Rand indices adapted archived clusterCrit R package Normalized mutual information (NMI) calculated using infotheo::entropy() Overall confusion matrix statistics caret::confusionMatrix: “Overall Accuracy”, “Cohen’s kappa”, “Information Rate”, “P-Value [Acc > NIR]” Averaged class-specific confusion matrix statistics: “Sensitivity”, “Specificity”, “PPV”, “NPV”, “Detection Rate”, “Accuracy”, “Balanced Accuracy”","code":""},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"consensus-clustering","dir":"Articles","previous_headings":"","what":"Consensus Clustering","title":"Cluster Analysis using `diceR`","text":"Monti et al. (2003) first introduced consensus clustering, idea use one clustering algorithm bootstrapped subsamples data. implement extensions consensus reached across subsamples across algorithms. final cluster assignment computed using statistical transformations ensemble cluster. base function package consensus_cluster(), outputs cluster assignments across subsamples algorithms, different k (number clusters). example, let’s say interested clustering hgsc data 3 4 clusters, using 80% resampling 5 replicates, clustering algorithms: Hierarchical Clustering, PAM, DIvisive ANAlysis Clustering (DIANA). Euclidean distance used algorithms. output 4-dimensional array: rows samples, columns different bootstrap subsample replicates, slices algorithms, “box” (4th dimension) different k. first cluster assignments replicate DIANA algorithm k = 3. Cluster Assignments DIANA, k = 3 Note unavoidable presence NAs used 80% subsampling. can problematic certain downstream ensemble methods, can impute missing values using K-Nearest Neighbours beforehand. might still NAs KNN decision threshold set. remaining missing values can imputed using majority voting. can carry analysis using either CC CC_imputed.","code":"CC <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,                         algorithms = c(\"hc\", \"pam\", \"diana\")) co <- capture.output(str(CC)) strwrap(co, width = 80) #> [1] \"int [1:100, 1:5, 1:3, 1:2] 1 1 NA NA NA 1 1 NA 1 NA ...\"                #> [2] \"- attr(*, \\\"dimnames\\\")=List of 4\"                                      #> [3] \"..$ : chr [1:100] \\\"TCGA.04.1331_PRO.C5\\\" \\\"TCGA.04.1332_MES.C1\\\"\"      #> [4] \"\\\"TCGA.04.1336_DIF.C4\\\" \\\"TCGA.04.1337_MES.C1\\\" ...\"                    #> [5] \"..$ : chr [1:5] \\\"R1\\\" \\\"R2\\\" \\\"R3\\\" \\\"R4\\\" ...\"                        #> [6] \"..$ : chr [1:3] \\\"HC_Euclidean\\\" \\\"PAM_Euclidean\\\" \\\"DIANA_Euclidean\\\"\" #> [7] \"..$ : chr [1:2] \\\"3\\\" \\\"4\\\"\" CC <- apply(CC, 2:4, impute_knn, data = hgsc, seed = 1) CC_imputed <- impute_missing(CC, hgsc, nk = 4) sum(is.na(CC)) #> [1] 2 sum(is.na(CC_imputed)) #> [1] 0"},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"consensus-functions","dir":"Articles","previous_headings":"","what":"Consensus Functions","title":"Cluster Analysis using `diceR`","text":"diceR provides functions retrieving useful summaries results consensus clustering. consensus_matrix() consensus_combine() consensus_evaluate()","code":""},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"compute-consensus-matrix-with-consensus_matrix","dir":"Articles","previous_headings":"Consensus Functions","what":"Compute consensus matrix with consensus_matrix()","title":"Cluster Analysis using `diceR`","text":"consensus matrix n n matrix, n number samples. element real-valued number 0 1 inclusive, representing proportion times two samples clustered together times samples chosen bootstrap resampling. diagonal one’s. Suppose wanted compute consensus matrix PAM, k = 4, visualize using graph_heatmap(): Consensus Matrix Heatmap","code":"pam.4 <- CC[, , \"PAM_Euclidean\", \"4\", drop = FALSE] cm <- consensus_matrix(pam.4) dim(cm) #> [1] 100 100 hm <- graph_heatmap(pam.4)"},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"combine-consensus-summaries-with-consensus_combine","dir":"Articles","previous_headings":"Consensus Functions","what":"Combine consensus summaries with consensus_combine()","title":"Cluster Analysis using `diceR`","text":"wish separately extract consensus matrices consensus classes every algorithm, consensus_combine() convenient wrapper . Setting element = \"matrix\" returns list consensus matrices. hand, setting element = \"class\" returns matrix rows samples, columns clustering assignments algorithm. Consensus Classes One can feed ccomb_class (instead CC) consensus_matrix() obtain consensus matrix across subsamples algorithms (k). situation might also arise initially decided using 3 clustering algorithms ensemble, later wish add additional algorithms analysis. consensus_combine() takes number ensemble objects (e.g. CC CC2) combines results. Consensus Classes KM added","code":"ccomb_matrix <- consensus_combine(CC, element = \"matrix\") ccomb_class <- consensus_combine(CC, element = \"class\") str(ccomb_matrix, max.level = 2) #> List of 2 #>  $ 3:List of 3 #>   ..$ HC_Euclidean   : num [1:100, 1:100] 1 0.8 1 0.8 1 1 1 1 1 1 ... #>   ..$ PAM_Euclidean  : num [1:100, 1:100] 1 1 0.2 0.8 0 1 1 0.8 0 0.2 ... #>   ..$ DIANA_Euclidean: num [1:100, 1:100] 1 0.8 0 0 0 1 1 0 0 1 ... #>  $ 4:List of 3 #>   ..$ HC_Euclidean   : num [1:100, 1:100] 1 0.6 1 0.6 1 1 1 1 1 0.8 ... #>   ..$ PAM_Euclidean  : num [1:100, 1:100] 1 1 0.2 0.8 0 0.8 0.8 0.6 0 0 ... #>   ..$ DIANA_Euclidean: num [1:100, 1:100] 1 0.8 0 0 0 1 1 0 0 1 ... # consensus matrix across subsamples and algorithms within k = 3 cm_k3 <- consensus_matrix(ccomb_class$`3`)  # consensus matrix across subsamples and algorithms within k = 4 cm_k4 <- consensus_matrix(ccomb_class$`4`)  # consensus matrix across subsamples and algorithms and k cm_all <- consensus_matrix(ccomb_class) CC2 <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,                          algorithms = \"km\") ccomb_class2 <- consensus_combine(CC, CC2, element = \"class\")"},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"evaluate-trim-and-reweigh-algorithms-with-consensus_evaluate","dir":"Articles","previous_headings":"Consensus Functions","what":"Evaluate, trim, and reweigh algorithms with consensus_evaluate()","title":"Cluster Analysis using `diceR`","text":"Internal cluster validation indices assess performance results taking account compactness separability clusters. choose variety indices compare collection clustering algorithms. use PAC (Proportion Ambiguous Clusters), proportion entries consensus matrix strictly lower (defaults 0) upper (defaults 1), give measure cluster stability. addition, reference class provided, calculate average PAC across algorithms within k, choose k greatest average PAC. reference class, k number distinct classes reference. Internal Indices k = 4 (continued ) Table continues see biclustering algorithm least ambiguous also well-clustered (high compactness separability). algorithms perform poorly deserve membership cluster ensemble. consider relative ranks algorithm across internal indices, compute sum. algorithms certain quantile sum rank trimmed (removed). default quantile 75%. trimming, can optionally choose reweigh algorithms based internal index magnitudes. course, take account direction optimality (higher better lower better). Algorithms reweighed fed consensus functions. done replicating algorithm scalar factor proportional weight. example, two algorithms B, given weight 80% B given weight 20%, make 4 copies 1 copy B. minimize computational time, total number copies algorithms upper bound 100. Without reweighing, algorithm given equal weight. return value shows algorithms kept, removed (), trimmed (potentially reweighed) cluster ensemble.","code":"ccomp <- consensus_evaluate(hgsc, CC, CC2, plot = FALSE) ctrim <- consensus_evaluate(hgsc, CC, CC2, trim = TRUE, reweigh = FALSE, n = 2) str(ctrim, max.level = 2) #> List of 5 #>  $ k       : int 4 #>  $ pac     :'data.frame':    2 obs. of  5 variables: #>   ..$ k              : chr [1:2] \"3\" \"4\" #>   ..$ HC_Euclidean   : num [1:2] 0.134 0.397 #>   ..$ PAM_Euclidean  : num [1:2] 0.458 0.451 #>   ..$ DIANA_Euclidean: num [1:2] 0.248 0.233 #>   ..$ KM             : num [1:2] 0.282 0.272 #>  $ ii      :List of 2 #>   ..$ 3:'data.frame':    4 obs. of  16 variables: #>   ..$ 4:'data.frame':    4 obs. of  16 variables: #>  $ ei      : NULL #>  $ trim.obj:List of 5 #>   ..$ alg.keep   : chr [1:2] \"HC_Euclidean\" \"KM\" #>   ..$ alg.remove : chr [1:2] \"PAM_Euclidean\" \"DIANA_Euclidean\" #>   ..$ rank.matrix:List of 1 #>   ..$ top.list   :List of 1 #>   ..$ E.new      :List of 1"},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"significance-testing","dir":"Articles","previous_headings":"","what":"Significance Testing","title":"Cluster Analysis using `diceR`","text":"test whether four statistically distinct clusters (k = 4) versus clusters (k = 1) using PAM algorithm, run sigclust 50 simulations generate p-value significance test. p-value 0.23, indicating sufficient evidence conclude four distinct clusters. Note use full hgsc data set example, underlying biological mechanisms may fully captured.","code":"set.seed(1) pam_4 <- ccomb_class2$`4`[, \"PAM_Euclidean\"] sig_obj <- sigclust(hgsc, k = 4, nsim = 100, labflag = 0, label = pam_4) co <- capture.output(str(sig_obj)) strwrap(co, width = 80) #>  [1] \"Formal class 'sigclust' [package \\\"sigclust\\\"] with 10 slots\"                #>  [2] \"..@ raw.data : num [1:100, 1:50] -0.0107 -0.7107 0.8815 -1.0851 -0.9322 ...\" #>  [3] \".. ..- attr(*, \\\"dimnames\\\")=List of 2\"                                      #>  [4] \".. .. ..$ : chr [1:100] \\\"TCGA.04.1331_PRO.C5\\\" \\\"TCGA.04.1332_MES.C1\\\"\"     #>  [5] \"\\\"TCGA.04.1336_DIF.C4\\\" \\\"TCGA.04.1337_MES.C1\\\" ...\"                         #>  [6] \".. .. ..$ : chr [1:50] \\\"ABAT\\\" \\\"ABHD2\\\" \\\"ACTB\\\" \\\"ACTR2\\\" ...\"            #>  [7] \"..@ veigval : num [1:50] 11.81 4.51 2.66 2.29 1.84 ...\"                      #>  [8] \"..@ vsimeigval: num [1:50] 11.81 4.51 2.66 2.29 1.84 ...\"                    #>  [9] \"..@ simbackvar: num 0.42\"                                                    #> [10] \"..@ icovest : num 2\"                                                         #> [11] \"..@ nsim : num 100\"                                                          #> [12] \"..@ simcindex : num [1:100] 0.624 0.692 0.664 0.672 0.637 ...\"               #> [13] \"..@ pval : num 0.23\"                                                         #> [14] \"..@ pvalnorm : num 0.206\"                                                    #> [15] \"..@ xcindex : num 0.63\""},{"path":"https://alinetalhouk.github.io/diceR/articles/overview.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Cluster Analysis using `diceR`","text":"Estivill-Castro, Vladimir (20 June 2002). “many clustering algorithms — Position Paper”. ACM SIGKDD Explorations Newsletter. 4 (1): 65–75. Brendan J. Frey; Delbert Dueck (2007). “Clustering passing messages data points”. Science. 315 (5814): 972–976. C. Fraley, . E. Raftery, T. B. Murphy L. Scrucca (2012). mclust Version 4 R: Normal Mixture Modeling Model-Based Clustering, Classification, Density Estimation. Technical Report . 597, Department Statistics, University Washington. Campello, R. J. G. B.; Moulavi, D.; Sander, J. (2013). Density-Based Clustering Based Hierarchical Density Estimates. Proceedings 17th Pacific-Asia Conference Knowledge Discovery Databases, PAKDD 2013, Lecture Notes Computer Science 7819, p. 160. Campello, Ricardo JGB, et al. “Hierarchical density estimates data clustering, visualization, outlier detection.” ACM Transactions Knowledge Discovery Data (TKDD) 10.1 (2015): 5. G. Govaert M. Nadif. Latent block model contingency table. Communications Statistics - Theory Methods, 39(3):416–425, 2010. Isa, Dino, V. P. Kallimani, Lam Hong Lee. “Using self organizing map clustering text documents.” Expert Systems Applications 36.5 (2009): 9584-9591. Andrew Y. Ng, Michael . Jordan, Yair Weiss Spectral Clustering: Analysis Algorithm Neural Information Processing Symposium 2001 Brunet J, Tamayo P, Golub TR Mesirov JP (2004). “Metagenes molecular pattern discovery using matrix factorization.” Proceedings National Academy Sciences United States America, 101(12), pp. 4164-9.","code":""},{"path":"https://alinetalhouk.github.io/diceR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Derek Chiu. Author, maintainer. Aline Talhouk. Author. Johnson Liu. Contributor, compiler.","code":""},{"path":"https://alinetalhouk.github.io/diceR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chiu D, Talhouk (2025). diceR: Diverse Cluster Ensemble R. R package version 3.0.0, https://alinetalhouk.github.io/diceR/, https://github.com/AlineTalhouk/diceR/.","code":"@Manual{,   title = {diceR: Diverse Cluster Ensemble in R},   author = {Derek Chiu and Aline Talhouk},   year = {2025},   note = {R package version 3.0.0,     https://alinetalhouk.github.io/diceR/},   url = {https://github.com/AlineTalhouk/diceR/}, }"},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Diverse Cluster Ensemble in R","text":"goal diceR provide systematic framework generating diverse cluster ensembles R. lot nuances cluster analysis consider. provide process suite functions tools implement systematic framework cluster discovery, guiding user generation diverse clustering solutions data, ensemble formation, algorithm selection arrival final consensus solution. additionally developed visual analytical validation tools help assessment final result. implemented wrapper function dice() allows user easily obtain results assess . Thus, package accessible end user limited statistical knowledge. Full access package available informaticians statisticians functions easily expanded. details can found companion paper published BMC Bioinformatics.","code":""},{"path":"https://alinetalhouk.github.io/diceR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Diverse Cluster Ensemble in R","text":"can install diceR CRAN : get latest development version GitHub:","code":"install.packages(\"diceR\") # install.packages(\"devtools\") devtools::install_github(\"AlineTalhouk/diceR\")"},{"path":"https://alinetalhouk.github.io/diceR/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Diverse Cluster Ensemble in R","text":"following example shows use main function package, dice(). data matrix hgsc contains subset gene expression measurements High Grade Serous Carcinoma Ovarian cancer patients Cancer Genome Atlas publicly available datasets. Samples rows, features columns. function runs package dice() function. specify (range ) nk clusters reps subsamples data containing 80% full samples. also specify clustering algorithms used ensemble functions used aggregated cons.funs. first cluster assignments shown : can also compare base algorithms cons.funs using internal evaluation indices:","code":"library(diceR) data(hgsc) obj <- dice(hgsc, nk = 4, reps = 5, algorithms = c(\"hc\", \"diana\"),             cons.funs = c(\"kmodes\", \"majority\")) knitr::kable(head(obj$clusters)) knitr::kable(obj$indices$ii$`4`)"},{"path":"https://alinetalhouk.github.io/diceR/index.html","id":"pipeline","dir":"","previous_headings":"","what":"Pipeline","title":"Diverse Cluster Ensemble in R","text":"figure visual schematic pipeline dice() implements. Ensemble Clustering pipeline. Please visit overview page detail.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/CSPA.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","title":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","text":"Performs hierarchical clustering stack consensus matrices obtain consensus class labels.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/CSPA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","text":"","code":"CSPA(E, k)"},{"path":"https://alinetalhouk.github.io/diceR/reference/CSPA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","text":"E array clustering results. k number clusters","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/CSPA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","text":"cluster assignments consensus class","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/CSPA.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","text":"Strehl, ., & Ghosh, J. (2002). Cluster ensembles—knowledge reuse framework combining multiple partitions. Journal machine learning research, 3(Dec), 583-617.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/CSPA.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/CSPA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cluster-based Similarity Partitioning Algorithm (CSPA) — CSPA","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] x <- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c(\"hc\", \"diana\"), progress = FALSE) CSPA(x, k = 4) #>   [1] 1 1 2 2 2 1 1 2 2 1 2 2 1 1 3 2 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 2 #>  [38] 2 3 2 1 2 2 2 2 2 2 2 1 1 2 2 1 1 1 2 2 1 2 1 1 1 2 1 1 4 1 2 2 1 2 1 1 2 #>  [75] 1 2 1 1 3 1 2 1 2 2 1 2 1 1 3 2 2 2 1 2 2 1 1 2 1 1"},{"path":"https://alinetalhouk.github.io/diceR/reference/LCA.html","id":null,"dir":"Reference","previous_headings":"","what":"Latent Class Analysis — LCA","title":"Latent Class Analysis — LCA","text":"Combine clustering results using latent class analysis.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/LCA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Latent Class Analysis — LCA","text":"","code":"LCA(E, is.relabelled = TRUE, seed = 1)"},{"path":"https://alinetalhouk.github.io/diceR/reference/LCA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Latent Class Analysis — LCA","text":"E matrix clusterings number rows equal number cases clustered, number columns equal clustering obtained different resampling data, third dimension different algorithms. Matrix may already two-dimensional. .relabelled logical; FALSE data relabelled using first clustering reference. seed random seed reproducibility","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/LCA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Latent Class Analysis — LCA","text":"vector cluster assignments based LCA","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/LCA.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Latent Class Analysis — LCA","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/LCA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Latent Class Analysis — LCA","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] cc <- consensus_cluster(dat, nk = 4, reps = 6, algorithms = \"pam\", progress = FALSE) table(LCA(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE)) #>  #>  1  2  3  4  #> 21 38  9 32"},{"path":"https://alinetalhouk.github.io/diceR/reference/LCE.html","id":null,"dir":"Reference","previous_headings":"","what":"Linkage Clustering Ensemble — LCE","title":"Linkage Clustering Ensemble — LCE","text":"Generate cluster assignment CTS, SRS, ASRS similarity matrix.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/LCE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linkage Clustering Ensemble — LCE","text":"","code":"LCE(E, k, dc = 0.8, R = 10, sim.mat = c(\"cts\", \"srs\", \"asrs\"))"},{"path":"https://alinetalhouk.github.io/diceR/reference/LCE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linkage Clustering Ensemble — LCE","text":"E array clustering results. error thrown missing values. impute_missing() can used beforehand. k requested number clusters dc decay constant CTS, SRS, ASRS matrix R number repetitions SRS matrix sim.mat similarity matrix; choices \"cts\", \"srs\", \"asrs\".","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/LCE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Linkage Clustering Ensemble — LCE","text":"vector containing cluster assignment either CTS, SRS, ASRS similarity matrices","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/LCE.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Linkage Clustering Ensemble — LCE","text":"Johnson Liu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/LCE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Linkage Clustering Ensemble — LCE","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] x <- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c(\"km\", \"hc\"), progress = FALSE) if (FALSE) { # \\dontrun{ LCE(E = x, k = 4, sim.mat = \"asrs\") } # }  x <- apply(x, 2:4, impute_knn, data = dat, seed = 1) x_imputed <- impute_missing(x, dat, nk = 4) LCE(E = x_imputed, k = 4, sim.mat = \"cts\") #>   [1] 1 1 1 1 1 1 1 1 1 2 1 1 2 1 3 1 1 2 1 1 2 1 2 1 1 1 2 1 2 1 1 1 1 1 2 2 1 #>  [38] 1 4 1 2 1 1 1 1 1 1 1 1 2 1 1 2 2 2 1 1 1 1 1 2 2 1 1 2 2 2 1 1 2 1 2 1 1 #>  [75] 1 1 1 1 4 2 1 1 1 1 1 1 1 1 3 1 1 1 2 1 1 2 2 1 2 1"},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":null,"dir":"Reference","previous_headings":"","what":"Proportion of Ambiguous Clustering — PAC","title":"Proportion of Ambiguous Clustering — PAC","text":"Given consensus matrix, returns proportion ambiguous clusters (PAC). robust way assess clustering performance.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Proportion of Ambiguous Clustering — PAC","text":"","code":"PAC(cm, lower = 0, upper = 1)"},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Proportion of Ambiguous Clustering — PAC","text":"cm consensus matrix. symmetric values 0 1. lower lower bound determines ambiguous upper upper bound determines ambiguous","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Proportion of Ambiguous Clustering — PAC","text":"PAC score used clustering performance. lower better, want minimal ambiguity amongst consensus.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Proportion of Ambiguous Clustering — PAC","text":"Since consensus matrix symmetric, look lower (upper) triangular matrix. proportion entries strictly lower upper PAC. perfect clustering, consensus matrix consist 0s 1s, PAC assessed (0, 1) interval perfect score 0. Using (0.1, 0.9) interval defining ambiguity common well. PAC , strictly speaking, internal validity index. Originally used choose optimal number clusters, use assess cluster stability. However, PAC still agnostic gold standard clustering result use like internal validity index.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Proportion of Ambiguous Clustering — PAC","text":"Senbabaoglu, Y., Michailidis, G., & Li, J. Z. (2014). Critical limitations consensus clustering class discovery. Scientific reports, 4.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Proportion of Ambiguous Clustering — PAC","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/PAC.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Proportion of Ambiguous Clustering — PAC","text":"","code":"set.seed(1) x <- replicate(100, rbinom(100, 4, 0.2)) y <- consensus_matrix(x) PAC(y, lower = 0.05, upper = 0.95) #> [1] 1"},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":null,"dir":"Reference","previous_headings":"","what":"Compactness Measure — compactness","title":"Compactness Measure — compactness","text":"Compute compactness validity index clustering result.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compactness Measure — compactness","text":"","code":"compactness(data, labels)"},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compactness Measure — compactness","text":"data dataset rows observations, columns variables labels vector cluster labels clustering result","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compactness Measure — compactness","text":"compactness score","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compactness Measure — compactness","text":"index agnostic reference clustering results, calculating cluster performance basis compactness separability. Smaller values indicate better clustering structure.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compactness Measure — compactness","text":"MATLAB function valid_compactness Simon Garrett LinkCluE","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compactness Measure — compactness","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/compactness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compactness Measure — compactness","text":"","code":"set.seed(1) E <- matrix(rep(sample(1:4, 1000, replace = TRUE)), nrow = 100, byrow =               FALSE) set.seed(1) dat <- as.data.frame(matrix(runif(1000, -10, 10), nrow = 100, byrow = FALSE)) compactness(dat, E[, 1]) #> [1] 25.32475"},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Consensus clustering — consensus_cluster","title":"Consensus clustering — consensus_cluster","text":"Runs consensus clustering across subsamples data, clustering algorithms, cluster sizes.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Consensus clustering — consensus_cluster","text":"","code":"consensus_cluster(   data,   nk = 2:4,   p.item = 0.8,   reps = 1000,   algorithms = NULL,   nmf.method = c(\"brunet\", \"lee\"),   hc.method = \"average\",   xdim = NULL,   ydim = NULL,   rlen = 200,   alpha = c(0.05, 0.01),   minPts = 5,   distance = \"euclidean\",   abs = TRUE,   prep.data = c(\"none\", \"full\", \"sampled\"),   scale = TRUE,   type = c(\"conventional\", \"robust\", \"tsne\"),   min.var = 1,   progress = TRUE,   seed.nmf = 123456,   seed.data = 1,   file.name = NULL,   time.saved = FALSE )"},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Consensus clustering — consensus_cluster","text":"data data matrix rows samples columns variables nk number clusters (k) requested; can specify single integer range integers compute multiple k p.item proportion items used subsampling within algorithm reps number subsamples algorithms vector clustering algorithms performing consensus clustering. Must number following: \"nmf\", \"hc\", \"diana\", \"km\", \"pam\", \"ap\", \"sc\", \"gmm\", \"block\", \"som\", \"cmeans\", \"hdbscan\". custom clustering algorithm can used. nmf.method specify NMF-based algorithms run. default \"brunet\" \"lee\" algorithms called. See NMF::nmf() details. hc.method agglomeration method hierarchical clustering. \"average\" method used default. Seestats::hclust() details. xdim x dimension SOM grid ydim y dimension SOM grid rlen number times complete data set presented SOM network. alpha SOM learning rate, vector two numbers indicating amount change. Default decline linearly 0.05 0.01 rlen updates. used batch algorithm. minPts minimum size clusters HDBSCAN. Default 5. distance vector distance functions. Defaults \"euclidean\". options given stats::dist(). custom distance function can used. abs used distance = c(\"spearman\", \"pearson\"). TRUE, absolute value first applied distance subtracting 1, e.g., use 1 - |SCD| instead 1 - SCD spearman correlation distance. prep.data Prepare data \"full\" dataset, \"sampled\" dataset, \"none\" (default). scale logical; data centered scaled? type use \"conventional\" measures (default), mean standard deviation used centering scaling, respectively. \"robust\" measures specified, median median absolute deviation (MAD) used. Alternatively, can apply \"tsne\" dimension reduction. min.var minimum variability measure threshold used filter feature space highly variable features. features minimum variability measure across samples greater min.var used. type = \"conventional\", standard deviation measure used, type = \"robust\", MAD measure used. progress logical; progress bar displayed? seed.nmf random seed use NMF-based algorithms seed.data seed use ensure algorithm operates set subsamples file.name NULL, returned array saved iteration well end function call rds object file.name file name. time.saved logical; TRUE, date saved appended file.name. applicable file.name NULL.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Consensus clustering — consensus_cluster","text":"array dimension nrow(x) reps length(algorithms) length(nk). cube array represents different k. slice cube matrix showing consensus clustering results algorithms. matrices row sample, column subsample. entry represents class membership. \"hdbscan\" part algorithms, include clustering array consensus result. Instead, report two summary statistics attributes: proportion outliers number clusters.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Consensus clustering — consensus_cluster","text":"See examples use custom algorithms distance functions. default clustering algorithms provided : \"nmf\": Nonnegative Matrix Factorization (using Kullback-Leibler Divergence Euclidean distance; See Note specifications.) \"hc\": Hierarchical Clustering \"diana\": DIvisive ANAlysis Clustering \"km\": K-Means Clustering \"pam\": Partition Around Medoids \"ap\": Affinity Propagation \"sc\": Spectral Clustering using Radial-Basis kernel function \"gmm\": Gaussian Mixture Model using Bayesian Information Criterion EM algorithm \"block\": Biclustering using latent block model \"som\": Self-Organizing Map (SOM) Hierarchical Clustering \"cmeans\": Fuzzy C-Means Clustering \"hdbscan\": Hierarchical Density-based Spatial Clustering Applications Noise (HDBSCAN) progress bar increments every unit reps.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Consensus clustering — consensus_cluster","text":"nmf.method options \"brunet\" (Kullback-Leibler Divergence) \"lee\" (Euclidean distance). \"hdbscan\" chosen algorithm use, results excluded rest consensus clusters. guarantee cluster assignment every sample clustered; often noise points outliers. addition, number distinct clusters may even equal nk.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Consensus clustering — consensus_cluster","text":"Derek Chiu, Aline Talhouk","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_cluster.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Consensus clustering — consensus_cluster","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50]  # Custom distance function manh <- function(x) {   stats::dist(x, method = \"manhattan\") }  # Custom clustering algorithm agnes <- function(d, k) {   return(as.integer(stats::cutree(cluster::agnes(d, diss = TRUE), k))) }  assign(\"agnes\", agnes, 1)  cc <- consensus_cluster(dat, reps = 6, algorithms = c(\"pam\", \"agnes\"), distance = c(\"euclidean\", \"manh\"), progress = FALSE) str(cc) #>  int [1:100, 1:6, 1:4, 1:3] 1 1 NA NA NA 1 1 NA 2 NA ... #>  - attr(*, \"dimnames\")=List of 4 #>   ..$ : chr [1:100] \"TCGA.04.1331_PRO.C5\" \"TCGA.04.1332_MES.C1\" \"TCGA.04.1336_DIF.C4\" \"TCGA.04.1337_MES.C1\" ... #>   ..$ : chr [1:6] \"R1\" \"R2\" \"R3\" \"R4\" ... #>   ..$ : chr [1:4] \"PAM_Euclidean\" \"PAM_Manh\" \"AGNES_Euclidean\" \"AGNES_Manh\" #>   ..$ : chr [1:3] \"2\" \"3\" \"4\""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_combine.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine algorithms — consensus_combine","title":"Combine algorithms — consensus_combine","text":"Combines results multiple objects consensus_cluster() outputs either consensus matrices consensus classes algorithms.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_combine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine algorithms — consensus_combine","text":"","code":"consensus_combine(..., element = c(\"matrix\", \"class\"))"},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_combine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine algorithms — consensus_combine","text":"... number objects outputted consensus_cluster() element either \"matrix\" \"class\" extract consensus matrix consensus class, respectively.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_combine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine algorithms — consensus_combine","text":"consensus_combine returns either list consensus matrices data frame showing consensus classes","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_combine.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Combine algorithms — consensus_combine","text":"function useful collecting summaries original results consensus_cluster combined single object. example, setting element = \"class\" returns matrix consensus cluster assignments, can visualized consensus matrix heatmap.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_combine.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Combine algorithms — consensus_combine","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_combine.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine algorithms — consensus_combine","text":"","code":"# Consensus clustering for multiple algorithms set.seed(911) x <- matrix(rnorm(500), ncol = 10) CC1 <- consensus_cluster(x, nk = 3:4, reps = 10, algorithms = \"ap\", progress = FALSE) CC2 <- consensus_cluster(x, nk = 3:4, reps = 10, algorithms = \"km\", progress = FALSE)  # Combine and return either matrices or classes y1 <- consensus_combine(CC1, CC2, element = \"matrix\") str(y1) #> List of 2 #>  $ 3:List of 2 #>   ..$ AP: num [1:50, 1:50] 1 0.429 0.167 0.429 1 ... #>   ..$ KM: num [1:50, 1:50] 1 0.625 0 0.429 1 ... #>  $ 4:List of 2 #>   ..$ AP: num [1:50, 1:50] 1 0.143 0.167 0 1 ... #>   ..$ KM: num [1:50, 1:50] 1 0.25 0 0.125 1 ... y2 <- consensus_combine(CC1, CC2, element = \"class\") str(y2) #> List of 2 #>  $ 3: int [1:50, 1:2] 1 1 2 1 1 1 2 1 3 2 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:2] \"AP\" \"KM\" #>  $ 4: int [1:50, 1:2] 1 2 3 2 1 2 3 3 4 3 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : NULL #>   .. ..$ : chr [1:2] \"AP\" \"KM\""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_evaluate.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate, trim, and reweigh algorithms — consensus_evaluate","title":"Evaluate, trim, and reweigh algorithms — consensus_evaluate","text":"Evaluates algorithms internal/external validation indices. Poor performing algorithms can trimmed ensemble. remaining algorithms can given weights use consensus functions.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_evaluate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate, trim, and reweigh algorithms — consensus_evaluate","text":"","code":"consensus_evaluate(   data,   ...,   cons.cl = NULL,   ref.cl = NULL,   k.method = NULL,   plot = FALSE,   trim = FALSE,   reweigh = FALSE,   n = 5,   lower = 0,   upper = 1 )"},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_evaluate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate, trim, and reweigh algorithms — consensus_evaluate","text":"data data matrix rows samples columns variables ... number objects outputted consensus_cluster() cons.cl matrix cluster assignments consensus functions kmodes majority_voting ref.cl reference class k.method determines method choose k reference class given. ref.cl NULL, k number distinct classes ref.cl. Otherwise input k.method chooses k. default use PAC choose best k(s). Specifying integer user-desired k override best k chosen PAC. Finally, specifying \"\" produce consensus results k. \"\" method implicitly performed one k used. plot logical; TRUE, graph_all called trim logical; TRUE, algorithms score low internal indices trimmed reweigh logical; TRUE, trimming poor performing algorithms, algorithm reweighed depending internal indices. n integer specifying top n algorithms keep trimming poor performing ones using Rank Aggregation. total number algorithms less n trimming done. lower lower bound determines ambiguous upper upper bound determines ambiguous","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_evaluate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate, trim, and reweigh algorithms — consensus_evaluate","text":"consensus_evaluate returns list following elements k: ref.cl NULL, number distinct classes reference; otherwise chosen k determined one giving largest mean PAC across algorithms pac: data frame showing PAC combination algorithm cluster size ii: list data frames k showing internal evaluation indices ei: data frame showing external evaluation indices k trim.obj: list 4 elements alg.keep: algorithms kept alg.remove: algorithms removed rank.matrix: matrix ranked algorithms every internal evaluation index top.list: final order ranked algorithms E.new: new version consensus_cluster data object","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_evaluate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluate, trim, and reweigh algorithms — consensus_evaluate","text":"function always returns internal indices. ref.cl NULL, external indices additionally shown. Relevant graphical displays also outputted. Algorithms ranked across internal indices using Rank Aggregation. top n algorithms kept, rest trimmed.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_evaluate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate, trim, and reweigh algorithms — consensus_evaluate","text":"","code":"# Consensus clustering for multiple algorithms set.seed(911) x <- matrix(rnorm(500), ncol = 10) CC <- consensus_cluster(x, nk = 3:4, reps = 10, algorithms = c(\"ap\", \"km\"), progress = FALSE)  # Evaluate algorithms on internal/external indices and trim algorithms: # remove those ranking low on internal indices set.seed(1) ref.cl <- sample(1:4, 50, replace = TRUE) z <- consensus_evaluate(x, CC, ref.cl = ref.cl, n = 1, trim = TRUE) str(z, max.level = 2) #> List of 5 #>  $ k       : int 4 #>  $ pac     :'data.frame':\t2 obs. of  3 variables: #>   ..$ k : chr [1:2] \"3\" \"4\" #>   ..$ AP: num [1:2] 0.505 0.48 #>   ..$ KM: num [1:2] 0.514 0.498 #>  $ ii      :List of 2 #>   ..$ 3:'data.frame':\t2 obs. of  16 variables: #>   ..$ 4:'data.frame':\t2 obs. of  16 variables: #>  $ ei      :List of 1 #>   ..$ 4:'data.frame':\t2 obs. of  19 variables: #>  $ trim.obj:List of 5 #>   ..$ alg.keep   : chr \"KM\" #>   ..$ alg.remove : chr \"AP\" #>   ..$ rank.matrix:List of 1 #>   ..$ top.list   :List of 1 #>   ..$ E.new      :List of 1"},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Consensus matrix — consensus_matrix","title":"Consensus matrix — consensus_matrix","text":"Returns (weighted) consensus matrix given data matrix","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Consensus matrix — consensus_matrix","text":"","code":"consensus_matrix(data, weights = NULL)"},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Consensus matrix — consensus_matrix","text":"data data matrix rows samples, columns replicates weights vector weights algorithm used meta-consensus clustering. Must length(weights) equal ncol(data).","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Consensus matrix — consensus_matrix","text":"consensus matrix","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Consensus matrix — consensus_matrix","text":"Given vector cluster assignments, first calculate connectivity matrix indicator matrix. connectivity matrix 1 samples cluster, 0 otherwise. indicator matrix 1 samples selected used subsample consensus clustering algorithm, 0 otherwise. Summation connectivity matrices indicator matrices performed different subsamples data. consensus matrix calculated dividing aggregated connectivity matrices aggregated indicator matrices. meta-consensus matrix desired, consensus classes different clustering algorithms aggregated, can construct weighted meta-consensus matrix using weights.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Consensus matrix — consensus_matrix","text":"consensus calculated bootstrap samples, every sample used replication. Thus, scenarios two samples never chosen together bootstrap samples. typically happens number replications small. coordinate consensus matrix pairs samples NaN 0 / 0 computation. entries coerced 0.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Consensus matrix — consensus_matrix","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/consensus_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Consensus matrix — consensus_matrix","text":"","code":"set.seed(2) x <- replicate(100, rbinom(100, 4, 0.2)) w <- rexp(100) w <- w / sum(w) cm1 <- consensus_matrix(x) cm2 <- consensus_matrix(x, weights = w)"},{"path":"https://alinetalhouk.github.io/diceR/reference/dice.html","id":null,"dir":"Reference","previous_headings":"","what":"Diverse Clustering Ensemble — dice","title":"Diverse Clustering Ensemble — dice","text":"Runs consensus clustering across subsamples, algorithms, number clusters (k).","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/dice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diverse Clustering Ensemble — dice","text":"","code":"dice(   data,   nk,   p.item = 0.8,   reps = 10,   algorithms = NULL,   k.method = NULL,   nmf.method = c(\"brunet\", \"lee\"),   hc.method = \"average\",   distance = \"euclidean\",   cons.funs = c(\"kmodes\", \"majority\", \"CSPA\", \"LCE\", \"LCA\"),   sim.mat = c(\"cts\", \"srs\", \"asrs\"),   prep.data = c(\"none\", \"full\", \"sampled\"),   min.var = 1,   seed = 1,   seed.data = 1,   trim = FALSE,   reweigh = FALSE,   n = 5,   evaluate = TRUE,   plot = FALSE,   ref.cl = NULL,   progress = TRUE )"},{"path":"https://alinetalhouk.github.io/diceR/reference/dice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diverse Clustering Ensemble — dice","text":"data data matrix rows samples columns variables nk number clusters (k) requested; can specify single integer range integers compute multiple k p.item proportion items used subsampling within algorithm reps number subsamples algorithms vector clustering algorithms performing consensus clustering. Must number following: \"nmf\", \"hc\", \"diana\", \"km\", \"pam\", \"ap\", \"sc\", \"gmm\", \"block\", \"som\", \"cmeans\", \"hdbscan\". custom clustering algorithm can used. k.method determines method choose k reference class given. ref.cl NULL, k number distinct classes ref.cl. Otherwise input k.method chooses k. default use PAC choose best k(s). Specifying integer user-desired k override best k chosen PAC. Finally, specifying \"\" produce consensus results k. \"\" method implicitly performed one k used. nmf.method specify NMF-based algorithms run. default \"brunet\" \"lee\" algorithms called. See NMF::nmf() details. hc.method agglomeration method hierarchical clustering. \"average\" method used default. Seestats::hclust() details. distance vector distance functions. Defaults \"euclidean\". options given stats::dist(). custom distance function can used. cons.funs consensus functions use. Current options \"kmodes\" (k-modes), \"majority\" (majority voting), \"CSPA\" (Cluster-based Similarity Partitioning Algorithm), \"LCE\" (linkage clustering ensemble), \"LCA\" (latent class analysis) sim.mat similarity matrix; choices \"cts\", \"srs\", \"asrs\". prep.data Prepare data \"full\" dataset, \"sampled\" dataset, \"none\" (default). min.var minimum variability measure threshold used filter feature space highly variable features. features minimum variability measure across samples greater min.var used. type = \"conventional\", standard deviation measure used, type = \"robust\", MAD measure used. seed random seed knn imputation reproducibility seed.data seed use ensure algorithm operates set subsamples trim logical; TRUE, algorithms score low internal indices trimmed reweigh logical; TRUE, trimming poor performing algorithms, algorithm reweighed depending internal indices. n integer specifying top n algorithms keep trimming poor performing ones using Rank Aggregation. total number algorithms less n trimming done. evaluate logical; TRUE (default), validity indices returned. Internal validity indices always computed. ref.cl NULL, external validity indices also computed. plot logical; TRUE, graph_all called summary evaluation heatmap ranked algorithms vs. internal validity indices plotted well. ref.cl reference class progress logical; progress bar displayed?","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/dice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Diverse Clustering Ensemble — dice","text":"list following elements E raw clustering ensemble object Eknn clustering ensemble object knn imputation used E Ecomp flattened ensemble object remaining missing entries imputed majority voting clusters final clustering assignment diverse clustering ensemble method indices evaluate = TRUE, shows cluster evaluation indices; otherwise NULL","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/dice.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Diverse Clustering Ensemble — dice","text":"three ways handle input data clustering via argument prep.data. default use raw data -(\"none\"). , can enact prepare_data() full dataset (\"full\"), bootstrap sampled datasets (\"sampled\").","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/dice.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Diverse Clustering Ensemble — dice","text":"Aline Talhouk, Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/dice.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Diverse Clustering Ensemble — dice","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following object is masked from ‘package:Biobase’: #>  #>     combine #> The following objects are masked from ‘package:BiocGenerics’: #>  #>     combine, intersect, setdiff, union #> The following object is masked from ‘package:MASS’: #>  #>     select #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union data(hgsc) dat <- hgsc[1:100, 1:50] ref.cl <- strsplit(rownames(dat), \"_\") %>%   purrr::map_chr(2) %>%   factor() %>%   as.integer() dice.obj <- dice(dat, nk = 4, reps = 5, algorithms = \"hc\", cons.funs = \"kmodes\", ref.cl = ref.cl, progress = FALSE) str(dice.obj, max.level = 2) #> List of 5 #>  $ E       : int [1:100, 1:5, 1, 1] 1 1 NA NA NA 1 1 NA 1 NA ... #>   ..- attr(*, \"dimnames\")=List of 4 #>  $ Eknn    : int [1:100, 1:5, 1, 1] 1 1 1 1 1 1 1 1 1 3 ... #>   ..- attr(*, \"dimnames\")=List of 4 #>  $ Ecomp   : num [1:100, 1:5, 1] 1 1 1 1 1 1 1 1 1 3 ... #>   ..- attr(*, \"dimnames\")=List of 3 #>  $ clusters: int [1:100, 1:2] 4 3 1 3 3 4 1 3 2 4 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>  $ indices :List of 5 #>   ..$ k   : int 4 #>   ..$ pac :'data.frame':\t1 obs. of  2 variables: #>   ..$ ii  :List of 1 #>   ..$ ei  :List of 1 #>   ..$ trim:List of 5"},{"path":"https://alinetalhouk.github.io/diceR/reference/diceR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"diceR: Diverse Cluster Ensemble in R — diceR-package","title":"diceR: Diverse Cluster Ensemble in R — diceR-package","text":"Performs cluster analysis using ensemble clustering framework, Chiu & Talhouk (2018) doi:10.1186/s12859-017-1996-y . Results diverse set algorithms pooled together using methods majority voting, K-Modes, LinkCluE, CSPA. options compare cluster assignments across algorithms using internal external indices, visualizations heatmaps, significance testing existence clusters.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/diceR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"diceR: Diverse Cluster Ensemble in R — diceR-package","text":"Maintainer: Derek Chiu dchiu@bccrc.ca Authors: Aline Talhouk .talhouk@ubc.ca contributors: Johnson Liu gliu@bccrc.ca [contributor, compiler]","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":null,"dir":"Reference","previous_headings":"","what":"External validity indices — external_validity","title":"External validity indices — external_validity","text":"External validity indices compare predicted clustering result reference class gold standard.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"External validity indices — external_validity","text":"","code":"ev_nmi(pred.lab, ref.lab, method = \"emp\")  ev_confmat(pred.lab, ref.lab)"},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"External validity indices — external_validity","text":"pred.lab predicted labels generated classifier ref.lab reference labels observations method method computing entropy. Can one \"emp\", \"mm\", \"shrink\", \"sg\".","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"External validity indices — external_validity","text":"ev_nmi returns normalized mutual information. ev_confmat returns tibble following summary statistics using yardstick::summary.conf_mat(): accuracy: Accuracy kap: Cohen's kappa sens: Sensitivity spec: Specificity ppv: Positive predictive value npv: Negative predictive value mcc: Matthews correlation coefficient j_index: Youden's J statistic bal_accuracy: Balanced accuracy detection_prevalence: Detection prevalence precision: alias ppv recall: alias sens f_meas: F Measure","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"External validity indices — external_validity","text":"ev_nmi calculates normalized mutual information ev_confmat calculates variety statistics associated confusion matrices. Accuracy, Cohen's kappa, Matthews correlation coefficient direct multiclass definitions, whereas metrics use macro-averaging.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"External validity indices — external_validity","text":"ev_nmi adapted infotheo::mutinformation()","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"External validity indices — external_validity","text":"Strehl , Ghosh J. Cluster ensembles: knowledge reuse framework combining multiple partitions. J. Mach. Learn. Res. 2002;3:583-617.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"External validity indices — external_validity","text":"Johnson Liu, Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/external_validity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"External validity indices — external_validity","text":"","code":"set.seed(1) E <- matrix(rep(sample(1:4, 1000, replace = TRUE)), nrow = 100, byrow =               FALSE) x <- sample(1:4, 100, replace = TRUE) y <- sample(1:4, 100, replace = TRUE) ev_nmi(x, y) #> [1] 0.05665824 ev_confmat(x, y) #> # A tibble: 13 × 3 #>    .metric              .estimator .estimate #>    <chr>                <chr>          <dbl> #>  1 accuracy             multiclass     0.36  #>  2 kap                  multiclass     0.137 #>  3 sens                 macro          0.349 #>  4 spec                 macro          0.785 #>  5 ppv                  macro          0.344 #>  6 npv                  macro          0.785 #>  7 mcc                  multiclass     0.138 #>  8 j_index              macro          0.134 #>  9 bal_accuracy         macro          0.567 #> 10 detection_prevalence macro          0.25  #> 11 precision            macro          0.344 #> 12 recall               macro          0.349 #> 13 f_meas               macro          0.345"},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":null,"dir":"Reference","previous_headings":"","what":"Graphical Displays — graphs","title":"Graphical Displays — graphs","text":"Graph cumulative distribution function (CDF) graphs, relative change area CDF curves, heatmaps, cluster assignment tracking plots.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graphical Displays — graphs","text":"","code":"graph_cdf(mat)  graph_delta_area(mat)  graph_heatmap(mat, main = NULL)  graph_tracking(cl)  graph_all(x)"},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graphical Displays — graphs","text":"mat x, list consensus matrices computed x faster results main heatmap title. NULL (default), titles taken names mat cl x, matrix consensus classes computed x faster results x object consensus_cluster()","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Graphical Displays — graphs","text":"Various plots graph_*{} functions. plots generated using ggplot, except graph_heatmap, uses NMF::aheatmap(). Colours used graph_heatmap graph_tracking utilize RColorBrewer::brewer.pal() palettes.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Graphical Displays — graphs","text":"graph_cdf plots CDF consensus matrices different algorithms. graph_delta_area calculates relative change area CDF curve algorithms. graph_heatmap generates consensus matrix heatmaps algorithm x. graph_tracking tracks cluster assignments change algorithms. graph_all wrapper runs graphing functions.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Graphical Displays — graphs","text":"https://stackoverflow.com/questions/4954507/calculate--area---curve","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Graphical Displays — graphs","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/graphs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Graphical Displays — graphs","text":"","code":"# Consensus clustering for 3 algorithms library(ggplot2) set.seed(911) x <- matrix(rnorm(80), ncol = 10) CC1 <- consensus_cluster(x, nk = 2:4, reps = 3, algorithms = c(\"hc\", \"pam\", \"km\"), progress = FALSE)  # Plot CDF p <- graph_cdf(CC1)   # Change y label and add colours p + labs(y = \"Probability\") + stat_ecdf(aes(colour = k)) + scale_color_brewer(palette = \"Set2\")   # Delta Area p <- graph_delta_area(CC1)   # Heatmaps with column side colours corresponding to clusters CC2 <- consensus_cluster(x, nk = 3, reps = 3, algorithms = \"hc\", progress = FALSE) graph_heatmap(CC2)   # Track how cluster assignments change between algorithms p <- graph_tracking(CC1)"},{"path":"https://alinetalhouk.github.io/diceR/reference/hgsc.html","id":null,"dir":"Reference","previous_headings":"","what":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","title":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","text":"489 samples measured 321 genes. Sample IDs row names gene names column names. data set used clustering HGSC subtypes prognostic significance. cluster assignments obtained TCGA indicated last six characters row name hgsc: MES.C1, IMM.C2, DIF.C4, PRO.C5","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/hgsc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","text":"","code":"hgsc"},{"path":"https://alinetalhouk.github.io/diceR/reference/hgsc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","text":"data frame 489 rows 321 columns.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_knn.html","id":null,"dir":"Reference","previous_headings":"","what":"K-Nearest Neighbours imputation — impute_knn","title":"K-Nearest Neighbours imputation — impute_knn","text":"non-missing cases indicate training set, missing cases indicate test set.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_knn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"K-Nearest Neighbours imputation — impute_knn","text":"","code":"impute_knn(x, data, seed = 123456)"},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_knn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K-Nearest Neighbours imputation — impute_knn","text":"x clustering object data data matrix seed random seed knn imputation reproducibility","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_knn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"K-Nearest Neighbours imputation — impute_knn","text":"object (potentially ) missing values imputed K-Nearest Neighbours.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_knn.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"K-Nearest Neighbours imputation — impute_knn","text":"consider 5 nearest neighbours minimum vote definite decision 3.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_knn.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"K-Nearest Neighbours imputation — impute_knn","text":"Aline Talhouk","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_knn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"K-Nearest Neighbours imputation — impute_knn","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] x <- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c(\"km\", \"hc\", \"diana\"), progress = FALSE) x <- apply(x, 2:4, impute_knn, data = dat, seed = 1)"},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_missing.html","id":null,"dir":"Reference","previous_headings":"","what":"Impute missing values — impute_missing","title":"Impute missing values — impute_missing","text":"Impute missing values bootstrapped subsampling","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_missing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Impute missing values — impute_missing","text":"","code":"impute_missing(E, data, nk)"},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_missing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Impute missing values — impute_missing","text":"E 4D array clusterings consensus_cluster. number rows equal number cases clustered, number columns equal clusterings obtained different resamplings data, third dimension different algorithms fourth dimension cluster sizes. data data matrix samples rows genes/features columns nk cluster size extract data (single value)","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_missing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Impute missing values — impute_missing","text":"flattened matrix consists one repetition, .e. column vector, function returns matrix clusterings complete cases imputed using majority voting, relabelled, chosen k.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_missing.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Impute missing values — impute_missing","text":"default output consensus_cluster undoubtedly contain NA entries replicate chooses random subset (replacement) samples. Missing values first imputed using impute_knn(). missing values guaranteed imputed KNN. See class::knn() details. Thus, remaining missing values imputed using majority voting.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_missing.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Impute missing values — impute_missing","text":"Aline Talhouk","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/impute_missing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Impute missing values — impute_missing","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] E <- consensus_cluster(dat, nk = 3:4, reps = 10, algorithms = c(\"hc\", \"km\", \"sc\"), progress = FALSE) sum(is.na(E)) #> [1] 1200 E_imputed <- impute_missing(E, dat, 4) sum(is.na(E_imputed)) #> [1] 0"},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":null,"dir":"Reference","previous_headings":"","what":"K-modes — k_modes","title":"K-modes — k_modes","text":"Combine clustering results using K-modes.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"K-modes — k_modes","text":"","code":"k_modes(E, is.relabelled = TRUE, seed = 1)"},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K-modes — k_modes","text":"E matrix clusterings number rows equal number cases clustered, number columns equal clustering obtained different resampling data, third dimension different algorithms. Matrix may already two-dimensional. .relabelled logical; FALSE data relabelled using first clustering reference. seed random seed reproducibility","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"K-modes — k_modes","text":"vector cluster assignments based k-modes","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"K-modes — k_modes","text":"Combine clustering results generated using different algorithms different data perturbations k-modes. method categorical data analog k-means clustering. Complete cases needed: .e. NAs. matrix contains NAs imputed majority voting (class relabeling).","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"K-modes — k_modes","text":"Luo, H., Kong, F., & Li, Y. (2006, August). Combining multiple clusterings via k-modes algorithm. International Conference Advanced Data Mining Applications (pp. 308-315). Springer, Berlin, Heidelberg.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"K-modes — k_modes","text":"Aline Talhouk","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/k_modes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"K-modes — k_modes","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] cc <- consensus_cluster(dat, nk = 4, reps = 6, algorithms = \"pam\", progress = FALSE) table(k_modes(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE)) #>  #>  1  2  3  4  #> 31 19 40 10"},{"path":"https://alinetalhouk.github.io/diceR/reference/majority_voting.html","id":null,"dir":"Reference","previous_headings":"","what":"Majority voting — majority_voting","title":"Majority voting — majority_voting","text":"Combine clustering results using majority voting.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/majority_voting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Majority voting — majority_voting","text":"","code":"majority_voting(E, is.relabelled = TRUE)"},{"path":"https://alinetalhouk.github.io/diceR/reference/majority_voting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Majority voting — majority_voting","text":"E matrix clusterings number rows equal number cases clustered, number columns equal clustering obtained different resampling data, third dimension different algorithms. Matrix may already two-dimensional. .relabelled logical; FALSE data relabelled using first clustering reference.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/majority_voting.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Majority voting — majority_voting","text":"vector cluster assignments based majority voting","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/majority_voting.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Majority voting — majority_voting","text":"Combine clustering results generated using different algorithms different data perturbations majority voting. class sample cluster label selected often across algorithms subsamples.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/diceR/reference/majority_voting.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Majority voting — majority_voting","text":"Aline Talhouk","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/majority_voting.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Majority voting — majority_voting","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] cc <- consensus_cluster(dat, nk = 4, reps = 6, algorithms = \"pam\", progress = FALSE) table(majority_voting(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE)) #>  #>  1  2  3  4  #> 40 32  9 19"},{"path":"https://alinetalhouk.github.io/diceR/reference/min_fnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Minimize Frobenius norm for between two matrices — min_fnorm","title":"Minimize Frobenius norm for between two matrices — min_fnorm","text":"Finds permutation matrix Frobenius norm another matrix minimized.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/min_fnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Minimize Frobenius norm for between two matrices — min_fnorm","text":"","code":"min_fnorm(A, B = diag(nrow(A)))"},{"path":"https://alinetalhouk.github.io/diceR/reference/min_fnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Minimize Frobenius norm for between two matrices — min_fnorm","text":"data matrix want permute B matrix whose distance permuted want minimize. default, B <- diag(nrow()), permutation maximizes trace .","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/min_fnorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Minimize Frobenius norm for between two matrices — min_fnorm","text":"Permuted matrix permutation closest B","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/min_fnorm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Minimize Frobenius norm for between two matrices — min_fnorm","text":"Finds permutation P ||PA - B|| minimum Frobenius norm. Uses linear-sum assignment problem (LSAP) solver package clue. default B identity matrix dimension, permutation maximizes trace. procedure useful constructing confusion matrix know true class labels predicted class want compare reference class.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/min_fnorm.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Minimize Frobenius norm for between two matrices — min_fnorm","text":"Ravi Varadhan: https://stat.ethz.ch/pipermail/r-help/2010-April/236664.html","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/min_fnorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Minimize Frobenius norm for between two matrices — min_fnorm","text":"","code":"set.seed(1) A <- matrix(sample(1:25, size = 25, rep = FALSE), 5, 5) min_fnorm(A) #> $pmat #>      [,1] [,2] [,3] [,4] [,5] #> [1,]   25   11   16    9    8 #> [2,]    1   22   19   17    3 #> [3,]    2    5   23   20   24 #> [4,]    4   14   10   15   13 #> [5,]    7   18    6   12   21 #>  #> $perm #> Optimal assignment: #> 1 => 1, 2 => 4, 3 => 5, 4 => 2, 5 => 3 #>  #> $ord #> [1] 1 4 5 2 3 #>"},{"path":"https://alinetalhouk.github.io/diceR/reference/pcn.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate and select null distributions on empirical gene-gene correlations — pcn","title":"Simulate and select null distributions on empirical gene-gene correlations — pcn","text":"Using principal component constructed sample space, simulate null distributions univariate Normal distributions using pcn_simulate. subset distributions chosen using pcn_select.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/pcn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate and select null distributions on empirical gene-gene correlations — pcn","text":"","code":"pcn_simulate(data, n.sim = 50)  pcn_select(data.sim, cl, type = c(\"rep\", \"range\"), int = 5)"},{"path":"https://alinetalhouk.github.io/diceR/reference/pcn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate and select null distributions on empirical gene-gene correlations — pcn","text":"data data matrix rows samples, columns features n.sim number simulated datasets simulate data.sim object pcn_simulate cl vector cluster memberships type select either representative dataset (\"rep\") range datasets (\"range\") int every int data sets median-ranked data.sim taken. Defaults 5.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/pcn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate and select null distributions on empirical gene-gene correlations — pcn","text":"pcn_simulate returns list length n.sim. element simulated matrix using \"Principal Component Normal\" (pcn) procedure. pcn_select returns list elements ranks: type = \"range\", ranks extracted dataset shown ind: index representative simulation dat: simulation data representation pcNormal","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/pcn.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate and select null distributions on empirical gene-gene correlations — pcn","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/pcn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate and select null distributions on empirical gene-gene correlations — pcn","text":"","code":"set.seed(9) A <- matrix(rnorm(300), nrow = 20) pc.dat <- pcn_simulate(A, n.sim = 50) cl <- sample(1:4, 20, replace = TRUE) pc.select <- pcn_select(pc.dat, cl, \"rep\")"},{"path":"https://alinetalhouk.github.io/diceR/reference/prepare_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare data for consensus clustering — prepare_data","title":"Prepare data for consensus clustering — prepare_data","text":"Perform feature selection dimension reduction remove noise variables.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/prepare_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare data for consensus clustering — prepare_data","text":"","code":"prepare_data(   data,   scale = TRUE,   type = c(\"conventional\", \"robust\", \"tsne\"),   min.var = 1 )"},{"path":"https://alinetalhouk.github.io/diceR/reference/prepare_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare data for consensus clustering — prepare_data","text":"data data matrix rows samples columns variables scale logical; data centered scaled? type use \"conventional\" measures (default), mean standard deviation used centering scaling, respectively. \"robust\" measures specified, median median absolute deviation (MAD) used. Alternatively, can apply \"tsne\" dimension reduction. min.var minimum variability measure threshold used filter feature space highly variable features. features minimum variability measure across samples greater min.var used. type = \"conventional\", standard deviation measure used, type = \"robust\", MAD measure used.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/prepare_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare data for consensus clustering — prepare_data","text":"dataset prepared usage consensus_cluster","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/prepare_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare data for consensus clustering — prepare_data","text":"can apply basic filtering method feature selection removes variables low signal (optionally) scales consensus clustering. , can use t-SNE dimension reduction transform data just two variables. lower-dimensional embedding allows algorithms hierarchical clustering achieve greater performance.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/prepare_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepare data for consensus clustering — prepare_data","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/prepare_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare data for consensus clustering — prepare_data","text":"","code":"set.seed(2) x <- replicate(10, rnorm(100)) x.prep <- prepare_data(x) dim(x) #> [1] 100  10 dim(x.prep) #> [1] 100   4"},{"path":"https://alinetalhouk.github.io/diceR/reference/relabel_class.html","id":null,"dir":"Reference","previous_headings":"","what":"Relabel classes to a standard — relabel_class","title":"Relabel classes to a standard — relabel_class","text":"Relabel clustering categories match standard minimizing Frobenius norm two labels.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/relabel_class.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relabel classes to a standard — relabel_class","text":"","code":"relabel_class(pred.cl, ref.cl)"},{"path":"https://alinetalhouk.github.io/diceR/reference/relabel_class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relabel classes to a standard — relabel_class","text":"pred.cl vector predicted cluster assignments ref.cl vector reference labels match ","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/relabel_class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Relabel classes to a standard — relabel_class","text":"vector relabeled cluster assignments","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/relabel_class.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Relabel classes to a standard — relabel_class","text":"Aline Talhouk","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/relabel_class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relabel classes to a standard — relabel_class","text":"","code":"set.seed(2) pred <- sample(1:4, 100, replace = TRUE) true <- sample(1:4, 100, replace = TRUE) relabel_class(pred, true) #>   [1] 3 2 1 1 4 4 3 3 3 4 3 3 1 2 3 2 1 4 4 1 2 2 2 4 2 3 1 3 4 4 2 1 3 1 4 4 3 #>  [38] 1 4 1 2 3 2 4 3 3 1 1 1 2 1 2 4 2 3 2 4 3 3 2 3 1 4 4 3 4 1 3 4 1 3 1 3 2 #>  [75] 1 1 2 4 4 4 4 3 1 2 1 1 1 4 2 2 3 2 4 1 3 4 3 3 3 1"},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":null,"dir":"Reference","previous_headings":"","what":"Significant Testing of Clustering Results — sigclust","title":"Significant Testing of Clustering Results — sigclust","text":"Uses SigClust K-Means algorithm assess significance clustering results.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Significant Testing of Clustering Results — sigclust","text":"","code":"sigclust(x, k, nsim, nrep = 1, labflag = 0, label = 0, icovest = 2)"},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Significant Testing of Clustering Results — sigclust","text":"x data matrix, samples rows features columns k cluster size test nsim number simulations nrep See sigclust::sigclust() details. labflag See sigclust::sigclust() details. label true class label. See sigclust::sigclust() details. icovest type covariance matrix estimation","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Significant Testing of Clustering Results — sigclust","text":"object class sigclust. See sigclust::sigclust() details.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Significant Testing of Clustering Results — sigclust","text":"function wrapper original sigclust::sigclust(), except additional parameter k allows testing number clusters. addition, default type covariance estimation also different.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Significant Testing of Clustering Results — sigclust","text":"Liu, Yufeng, Hayes, David Neil, Nobel, Andrew Marron, J. S, 2008, Statistical Significance Clustering High-Dimension, Low-Sample Size Data, Journal American Statistical Association 103(483) 1281–1293.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Significant Testing of Clustering Results — sigclust","text":"Hanwen Huang: hanwenh@email.unc.edu; Yufeng Liu: yfliu@email.unc.edu; J. S. Marron: marron@email.unc.edu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/sigclust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Significant Testing of Clustering Results — sigclust","text":"","code":"data(hgsc) dat <- hgsc[1:100, 1:50] nk <- 4 cc <- consensus_cluster(dat, nk = nk, reps = 5, algorithms = \"pam\", progress = FALSE) cl.mat <- consensus_combine(cc, element = \"class\") lab <- cl.mat$`4`[, 1] set.seed(1) str(sigclust(x = dat, k = nk, nsim = 50, labflag = 1, label = lab)) #> Formal class 'sigclust' [package \"sigclust\"] with 10 slots #>   ..@ raw.data  : num [1:100, 1:50] -0.0107 -0.7107 0.8815 -1.0851 -0.9322 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : chr [1:100] \"TCGA.04.1331_PRO.C5\" \"TCGA.04.1332_MES.C1\" \"TCGA.04.1336_DIF.C4\" \"TCGA.04.1337_MES.C1\" ... #>   .. .. ..$ : chr [1:50] \"ABAT\" \"ABHD2\" \"ACTB\" \"ACTR2\" ... #>   ..@ veigval   : num [1:50] 11.81 4.51 2.66 2.29 1.84 ... #>   ..@ vsimeigval: num [1:50] 11.81 4.51 2.66 2.29 1.84 ... #>   ..@ simbackvar: num 0.42 #>   ..@ icovest   : num 2 #>   ..@ nsim      : num 50 #>   ..@ simcindex : num [1:50] 0.647 0.673 0.65 0.652 0.59 ... #>   ..@ pval      : num 0.76 #>   ..@ pvalnorm  : num 0.776 #>   ..@ xcindex   : num 0.667"},{"path":"https://alinetalhouk.github.io/diceR/reference/similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Similarity Matrices — similarity","title":"Similarity Matrices — similarity","text":"cts computes connected triple based similarity matrix, srs computes simrank based similarity matrix, asrs computes approximated simrank based similarity matrix.","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Similarity Matrices — similarity","text":"","code":"cts(E, dc)  srs(E, dc, R)  asrs(E, dc)"},{"path":"https://alinetalhouk.github.io/diceR/reference/similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Similarity Matrices — similarity","text":"E N M matrix cluster ensembles dc decay factor, ranges 0 1 inclusive R number iterations srs","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Similarity Matrices — similarity","text":"N N CTS, SRS, ASRS matrix","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/similarity.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Similarity Matrices — similarity","text":"MATLAB functions cts, srs, asrs package LinkCluE Simon Garrett","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/similarity.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Similarity Matrices — similarity","text":"Johnson Liu, Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/diceR/reference/similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Similarity Matrices — similarity","text":"","code":"set.seed(1) E <- matrix(rep(sample(1:4, 800, replace = TRUE)), nrow = 100) CTS <- cts(E = E, dc = 0.8) SRS <- srs(E = E, dc = 0.8, R = 3) ASRS <- asrs(E = E, dc = 0.8) purrr::walk(list(CTS, SRS, ASRS), str) #>  num [1:100, 1:100] 1 0.888 0.792 0.811 0.715 ... #>  num [1:100, 1:100] 1 0.06901 0.03048 0.04348 0.00553 ... #>  num [1:100, 1:100] 1 0.638 0.634 0.638 0.653 ..."},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-300","dir":"Changelog","previous_headings":"","what":"diceR 3.0.0","title":"diceR 3.0.0","text":"errors running tests clang-UBSAN gcc-UBSAN builds, due usage functions clusterSim. Reinstate internal external functions clusterCrit package, longer archived. Remove deprecated function usage","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-220","dir":"Changelog","previous_headings":"","what":"diceR 2.2.0","title":"diceR 2.2.0","text":"CRAN release: 2024-01-22 New abs argument consensus_cluster(): control whether apply absolute value Spearman Pearson correlation matrices subtracting one (@tiagochst, #161) New distance matrix option, distance = \"pearson\" Use blockcluster instead mixedClust latter now archived Update tests consensus_cluster() skip suggested packages installed","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-210","dir":"Changelog","previous_headings":"","what":"diceR 2.1.0","title":"diceR 2.1.0","text":"CRAN release: 2023-09-28 Sort cluster sizes k correctly relative change area CDF curve (@IgnatiusPang, #167) consensus matrix CDF graphs Replace deprecated aes_ calls tidyeval idioms Pass p.item seed.data arguments dice() (#162, #165)","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-200","dir":"Changelog","previous_headings":"","what":"diceR 2.0.0","title":"diceR 2.0.0","text":"CRAN release: 2023-03-11 Internal external validity indices refactored avoid using helper functions clusterCrit package, scheduled archived. Please adapt code extracts deprecated validity indices, described . Calinski-Harabasz index now calculated using clusterSim::index.G1() Dunn index now calculated using clValid::dunn() Gamma index now calculated using clusterSim::index.G2() C-index now calculated using clusterSim::index.C() Davies-Bouldin now calculated using clusterSim::index.DB() SD index now calculated correctly using clv::clv.SD() helper functions clv. Previously total separation clusters returned. S_Dbw index now calculated using clv::clv.SDbw() helper functions clv Rousseeuw’s Silhouette now calculated using clusterSim::index.S() PBM, Tau, McClain-Rao, Ray-Turi, G-plus indices removed equivalent implementations packages found. may reinstated future. external validity indices now calculated manually using counts concordance matrix (Hubert, Jaccard, McNemar, Rand indices) Best index value (maximum minimum) now calculated manually","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-122","dir":"Changelog","previous_headings":"","what":"diceR 1.2.2","title":"diceR 1.2.2","text":"CRAN release: 2022-09-29 Pass lower upper parameters PAC() consensus_evaluate() (#160)","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-121","dir":"Changelog","previous_headings":"","what":"diceR 1.2.1","title":"diceR 1.2.1","text":"CRAN release: 2022-08-16 References added k_modes() CSPA() consensus functions (#157) Update roxygen2 avoid HTML5 documentation notes Use mixedClust instead blockcluster co-clustering since latter keeps getting archived","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-120","dir":"Changelog","previous_headings":"","what":"diceR 1.2.0","title":"diceR 1.2.0","text":"CRAN release: 2022-05-13 Use testthat::skip_if_not_installed() run tests conditionally using packages Suggests Use roxygen tag @examplesIf rlang::is_installed() run examples conditionally using packages Suggests","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-110","dir":"Changelog","previous_headings":"","what":"diceR 1.1.0","title":"diceR 1.1.0","text":"CRAN release: 2021-07-23 Reinstate blockcluster package active maintenance ","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-104","dir":"Changelog","previous_headings":"","what":"diceR 1.0.4","title":"diceR 1.0.4","text":"CRAN release: 2021-06-04 Suppress new names messages transformed NMF data Flattened matrices include 4th dimension clustering array","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-103","dir":"Changelog","previous_headings":"","what":"diceR 1.0.3","title":"diceR 1.0.3","text":"CRAN release: 2021-04-17 Add package logo using hexSticker Package blockcluster archived, remove Suggests","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-102","dir":"Changelog","previous_headings":"","what":"diceR 1.0.2","title":"diceR 1.0.2","text":"CRAN release: 2021-03-18 Remove deprecated context() tests Add RColorBrewer Suggests cross referenced ?graphs Add trailing slash URLs DESCRIPTION Remove tibble Imports, longer used","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-101","dir":"Changelog","previous_headings":"","what":"diceR 1.0.1","title":"diceR 1.0.1","text":"CRAN release: 2021-01-30 Suppress warnings clValid::connectivity() called regarding comparison one class. Since R-4.0.0, matrix object also inherits class array algii_heatmap(), object ii already row names passed columns named vectors (issue also identified #148, thanks @romainfrancois)","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-100","dir":"Changelog","previous_headings":"","what":"diceR 1.0.0","title":"diceR 1.0.0","text":"CRAN release: 2020-07-07","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"decreased-dependencies-1-0-0","dir":"Changelog","previous_headings":"","what":"Decreased dependencies","title":"diceR 1.0.0","text":"following steps taken minimize dependencies ensure diceR can still run R 3.5: Removed cli RColorBrewer Imports Moved apcluster, blockcluster, cluster, dbscan, e1071, kernlab, kohonen Suggests, use specific clustering algorithms conditionally. mclust needs Imports mclust::mclustBIC() needs imported Moved sigclust Suggests, use within sigclust() conditionally Moved progress Suggests, use within consensus_cluster() conditionally Moved poLCA Suggests, use within dice() conditionally Moved Rtsne Suggests, use within prepare_data() conditionally Removed old dependency grDevices Imports Set minimum version R (>= 3.5) klaR dependency questionr ev_confmat(), use yardstick::conf_mat() instead caret::confusionMatrix(). caret many dependencies, best avoid using graph_heatmap(), use NMF::aheatmap() instead gplots::heatmap.2(). gplots depends caTools, now relies R (>= 3.6) consensus_cluster(), use stringr::str_to_title() instead Hmisc::capitalize(). Hmisc depends latticeExtra, now relies R (>= 3.6) graph_delta_area(), use base solution instead flux::auc(). flux also depends caTools prepare_data(), use implementation quantable::robustscale() former function’s defaults. quantable also depends caTools Specify Bioconductor installation Travis AppVeyor since NMF now Imports Biobase","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"minor-improvements-and-bug-fixes-1-0-0","dir":"Changelog","previous_headings":"","what":"Minor improvements and bug fixes","title":"diceR 1.0.0","text":"Remove suppressWarnings(RNGversion(\"3.5.0\")) updating R version Run LCA() unit test imputed clustering object Remove internal validity measures Inf entries consensus_reweigh() Use cleaner, robust method removing Rplots.pdf running test-graphs.R Ensure column binding purrr::map_dfc() consensus_rank() Replaced dplyr::bind_cols() purrr::flatten_dfc() suppress warning “Outer names allowed unnamed scalar” get_cdf() update roxygen docs","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-060","dir":"Changelog","previous_headings":"","what":"diceR 0.6.0","title":"diceR 0.6.0","text":"CRAN release: 2019-07-25 Remove deprecated dplyr functions use .data pronoun k-means clustering support distance matrices input (@jerryji1993, #139) Add LCA consensus function (@philstraforelli, #137)","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-052","dir":"Changelog","previous_headings":"","what":"diceR 0.5.2","title":"diceR 0.5.2","text":"CRAN release: 2019-03-08 Fix length > 1 coercion logical error consensus_evaluate() due comparisons using || operator Add suppressWarnings(RNGversion(\"3.5.0\")) call set.seed() examples, tests, vignette use old RNG sampling Use .covrignore exclude zzz.R considered code coverage Use dplyr version >= 0.7.5 ensure bind_rows() works Fixed bug scaled matrix using “robust” method prepare_data() nested data element (@AlineTalhouk, #134)","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-051","dir":"Changelog","previous_headings":"","what":"diceR 0.5.1","title":"diceR 0.5.1","text":"CRAN release: 2018-06-11 Add parameter hc.method dice consensus_cluster pass method parameter stats::hclust (@JakeNel28, #130) Remove dependencies largeVis: package archived","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-050","dir":"Changelog","previous_headings":"","what":"diceR 0.5.0","title":"diceR 0.5.0","text":"CRAN release: 2018-05-05 Revert back using NMF since NNLM archived NMF back active maintenance. Choose fuzzifier m cmeans using Equation 5 https://academic.oup.com/bioinformatics/article/26/22/2841/227572 (thanks @Asduveneck)","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-040","dir":"Changelog","previous_headings":"","what":"diceR 0.4.0","title":"diceR 0.4.0","text":"CRAN release: 2018-02-22 Replace code depended NMF NNLM pheatmap: CRAN notified NMF archived inactive maintenance Update .yml files default templates","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-032","dir":"Changelog","previous_headings":"","what":"diceR 0.3.2","title":"diceR 0.3.2","text":"CRAN release: 2018-01-14 Fix bug consensus_cluster() custom algorithms excluded output (thanks @phiala) Use markdown language documentation Various performance improvements code simplifications","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-031","dir":"Changelog","previous_headings":"","what":"diceR 0.3.1","title":"diceR 0.3.1","text":"CRAN release: 2017-12-12 Suppress success/fail message printout fix input data matrix block clustering Fix bug algii_heatmap() k.method = \"\" dice() Fix bug calculating internal indices data categorical variables (thanks Kurt Salmela)","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-030","dir":"Changelog","previous_headings":"","what":"diceR 0.3.0","title":"diceR 0.3.0","text":"CRAN release: 2017-11-29 Updated object output names consensus_evaluate() Fix unit test test-dice.R R-devel Add internal function: ranked algorithms vs internal validity indices heatmap graph Fix bugs graph_cdf(), graph_tracking() one k selected Progress messages dice() Fix bug consensus_evaluate() algorithm NA PAC values","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-020","dir":"Changelog","previous_headings":"","what":"diceR 0.2.0","title":"diceR 0.2.0","text":"CRAN release: 2017-09-29 New dimension reduction methods: t-SNE, largeVis (@dustin21) Better annotated progress bar using progress package Speed operation transforms matrix become “NMF-ready” Simplify saving mechanism consensus_cluster() file.name needs specified, save parameter removed New algorithms: SOM, Fuzzy C-Means, DBSCAN (@dustin21, #118) Added significance testing section vignette Fixed direction optimization: compactness connectivity minimized","code":""},{"path":"https://alinetalhouk.github.io/diceR/news/index.html","id":"dicer-010","dir":"Changelog","previous_headings":"","what":"diceR 0.1.0","title":"diceR 0.1.0","text":"CRAN release: 2017-06-21 New submission CRAN accepted June 21, 2017","code":""}]
