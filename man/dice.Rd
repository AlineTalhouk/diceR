% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dice.R
\name{dice}
\alias{dice}
\title{Diverse Clustering Ensemble}
\usage{
dice(data, nk, reps = 10, algorithms = NULL, nmf.method = c("brunet",
  "lee"), distance = "euclidean", cons.funs = c("kmodes", "CSPA",
  "majority", "LCE"), sim.mat = c("cts", "srs", "asrs"),
  prep.data = c("full", "sampled", "none"), min.sd = 1, seed = 1,
  trim = FALSE, reweigh = FALSE, evaluate = TRUE, plot = FALSE,
  ref.cl = NULL, progress = TRUE)
}
\arguments{
\item{data}{matrix with rows as observations, columns as variables}

\item{nk}{number of clusters (k) requested; can specify a single integer or a
range of integers to compute multiple k}

\item{reps}{number of data subsamples to generate. See 
\code{\link{consensus_cluster}} for details.}

\item{algorithms}{clustering algorithms to be used in the ensemble. Current 
options are "nmf", "hc", "diana", "km", "pam", "ap", "sc", "gmm", "block".
See \code{\link{consensus_cluster}} for details.}

\item{nmf.method}{specify NMF-based algorithms to run. By default the 
"brunet" and "lee" algorithms are called. See
\code{\link{consensus_cluster}} for details.}

\item{distance}{a vector of distance functions. Defaults to "euclidean". Can 
use a custom distance function. See \code{\link{consensus_cluster}} for
details.}

\item{cons.funs}{consensus functions to use. Current options are "kmodes" 
(k-modes), "majority" (majority voting), "CSPA" (Cluster-based Similarity 
Partitioning Algorithm), "LCE" (linkage clustering ensemble)}

\item{sim.mat}{type of similarity matrix. One of "cts", "srs", "asrs. See 
\code{\link{LCE}} for details.}

\item{prep.data}{Prepare the data on the "full" dataset (default), the
"sampled" dataset, or "none".}

\item{min.sd}{minimum standard deviation threshold. See details.}

\item{seed}{seed used for imputation}

\item{trim}{logical; if \code{TRUE}, the number of algorithms in 
\code{algorithms} is reduced based on internal validity index performance 
prior to consensus clustering by \code{cons.funs}. Defaults to 
\code{FALSE}.}

\item{reweigh}{logical; if \code{TRUE}, algorithms are reweighted based on 
internal validity index performance after trimming. Well-performing 
algorithms are given higher weight prior to consensus clustering by 
\code{cons.funs}. Defaults to \code{FALSE}. Ignored if \code{trim = FALSE}.}

\item{evaluate}{logical; if \code{TRUE} (default), validity indices are 
returned. Internal validity indices are always computed. If \code{ref.cl} 
is not \code{NULL}, then external validity indices will also be computed.}

\item{plot}{logical; if \code{TRUE}, \code{\link{graph_all}} is called and 
relevant graphs are outputted. Ignored if \code{evaluate = FALSE}.}

\item{ref.cl}{reference class; a vector of length equal to the number of 
observations.}

\item{progress}{logical; if \code{TRUE} (default), progress bar is shown.}
}
\value{
A final clustering assignment from the diverse clustering ensemble 
  method.
}
\description{
Runs consensus clustering across subsamples, algorithms, and number of 
clusters (k).
}
\details{
The \code{min.sd} argument is used to filter the feature space for only 
highly variable features. Only features with a standard deviation across all 
samples greater than \code{min.sd} will be used.
}
\examples{
library(dplyr)
data(hgsc)
dat <- t(hgsc[, -1])
ref.cl <- data.frame(initCol = rownames(dat)) \%>\%
tidyr::separate(initCol,
                into = c("patientID", "Class"),
                sep = "_") \%>\% 
  magrittr::use_series(Class) \%>\% 
  factor() \%>\% 
  as.integer()
dice.obj <- dice(dat, nk = 4, reps = 3, algorithms = "hc", cons.funs =
"kmodes", ref.cl = ref.cl)
str(dice.obj, max.level = 2)
}
\author{
Aline Talhouk, Derek Chiu
}
