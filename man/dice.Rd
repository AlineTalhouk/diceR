% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dice.R
\name{dice}
\alias{dice}
\title{Diverse Clustering Ensemble}
\usage{
dice(data, nk, reps = 10, algorithms = NULL, nmf.method = c("brunet",
  "lee"), distance = "euclidean", cons.funs = c("kmodes", "CSPA",
  "majority", "LCE"), sim.mat = c("cts", "srs", "asrs"),
  prep.data = c("none", "full", "sampled"), min.var = 1, seed = 1,
  trim = FALSE, reweigh = FALSE, evaluate = TRUE, plot = FALSE,
  ref.cl = NULL, progress = TRUE)
}
\arguments{
\item{data}{matrix with rows as observations, columns as variables}

\item{nk}{number of clusters (k) requested; can specify a single integer or a
range of integers to compute multiple k}

\item{reps}{number of data subsamples to generate. See 
\code{\link{consensus_cluster}} for details.}

\item{algorithms}{clustering algorithms to be used in the ensemble. Current 
options are "nmf", "hc", "diana", "km", "pam", "ap", "sc", "gmm", "block".
See \code{\link{consensus_cluster}} for details.}

\item{nmf.method}{specify NMF-based algorithms to run. By default the 
"brunet" and "lee" algorithms are called. See
\code{\link{consensus_cluster}} for details.}

\item{distance}{a vector of distance functions. Defaults to "euclidean". Can 
use a custom distance function. See \code{\link{consensus_cluster}} for
details.}

\item{cons.funs}{consensus functions to use. Current options are "kmodes" 
(k-modes), "majority" (majority voting), "CSPA" (Cluster-based Similarity 
Partitioning Algorithm), "LCE" (linkage clustering ensemble)}

\item{sim.mat}{type of similarity matrix. One of "cts", "srs", "asrs. See 
\code{\link{LCE}} for details.}

\item{prep.data}{Prepare the data on the "full" dataset, the
"sampled" dataset, or "none" (default). See Details.}

\item{min.var}{minimum variability measure threshold. See
\code{\link{prepare_data}}.}

\item{seed}{seed used for imputation}

\item{trim}{logical; if \code{TRUE}, the number of algorithms in 
\code{algorithms} is reduced based on internal validity index performance 
prior to consensus clustering by \code{cons.funs}. Defaults to 
\code{FALSE}.}

\item{reweigh}{logical; if \code{TRUE}, algorithms are reweighted based on 
internal validity index performance after trimming. Well-performing 
algorithms are given higher weight prior to consensus clustering by 
\code{cons.funs}. Defaults to \code{FALSE}. Ignored if \code{trim = FALSE}.}

\item{evaluate}{logical; if \code{TRUE} (default), validity indices are 
returned. Internal validity indices are always computed. If \code{ref.cl} 
is not \code{NULL}, then external validity indices will also be computed.}

\item{plot}{logical; if \code{TRUE}, \code{\link{graph_all}} is called and 
relevant graphs are outputted. Ignored if \code{evaluate = FALSE}.}

\item{ref.cl}{reference class; a vector of length equal to the number of 
observations.}

\item{progress}{logical; if \code{TRUE} (default), progress bar is shown.}
}
\value{
A list with the following elements
\item{E}{raw clustering ensemble object}
\item{Eknn}{clustering ensemble object with knn imputation used on \code{E}}
\item{Ecomp}{flattened ensemble object with remaining missing entries imputed
by majority voting}
\item{clusters}{final clustering assignment from the diverse clustering
ensemble method}
\item{indices}{if \code{evaluate = TRUE}, shows cluster evaluation indices;
otherwise \code{NULL}}
}
\description{
Runs consensus clustering across subsamples, algorithms, and number of 
clusters (k).
}
\details{
There are three ways to handle the input data before clustering via argument
\code{prep.data}. The default is to use the raw data as-is ("none"). Or, we
can enact \code{\link{prepare_data}} on the full dataset ("full"), or the
bootstrap sampled datasets ("sampled").
}
\examples{
library(dplyr)
data(hgsc)
dat <- t(hgsc[, -1])[1:100, 1:50]
ref.cl <- data.frame(initCol = rownames(dat)) \%>\%
tidyr::separate(initCol,
                into = c("patientID", "Class"),
                sep = "_") \%>\%
  magrittr::use_series(Class) \%>\%
  factor() \%>\%
  as.integer()
dice.obj <- dice(dat, nk = 4, reps = 5, algorithms = "hc", cons.funs =
"kmodes", ref.cl = ref.cl)
str(dice.obj, max.level = 2)
}
\author{
Aline Talhouk, Derek Chiu
}
